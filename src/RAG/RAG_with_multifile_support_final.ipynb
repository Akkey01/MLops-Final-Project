{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b31f6e35-b43a-45f7-85c2-5e13b77b092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-docx pdfplumber beautifulsoup4 html2text --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c90c1084-2ac3-4226-8f34-335cbfeba950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c6aa64-3a17-4fd5-a9ea-e58ff9ee809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_from_xml(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Extract all text from the XML (adjust tag logic if needed)\n",
    "    text_chunks = []\n",
    "    for elem in root.iter():\n",
    "        if elem.text and elem.text.strip():\n",
    "            text_chunks.append(elem.text.strip())\n",
    "    \n",
    "    return \" \".join(text_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0037c0-c692-4839-8bea-2e7ddce1a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_summaries(folder_path):\n",
    "    summaries = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".abssumm.xml\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            summary_text = extract_summary_from_xml(full_path)\n",
    "            if summary_text:\n",
    "                summaries.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"text\": summary_text\n",
    "                })\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6876c084",
   "metadata": {},
   "source": [
    "\n",
    "## 📥 Added Support for Loading Multi-format Files (PDF, DOCX, TXT, HTML)\n",
    "\n",
    "This section allows loading multiple document formats and standardizing them into plain text for chunking and embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace6634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "def extract_text_from_pdf(path):\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        return \"\\n\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
    "\n",
    "def extract_text_from_docx(path):\n",
    "    doc = Document(path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text_from_html(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        return html2text.html2text(soup.prettify())\n",
    "\n",
    "def extract_text_from_txt(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_any_file(filepath):\n",
    "    ext = os.path.splitext(filepath)[-1].lower()\n",
    "    if ext == '.pdf':\n",
    "        return extract_text_from_pdf(filepath)\n",
    "    elif ext == '.docx':\n",
    "        return extract_text_from_docx(filepath)\n",
    "    elif ext == '.html':\n",
    "        return extract_text_from_html(filepath)\n",
    "    elif ext in ['.txt', '.md']:\n",
    "        return extract_text_from_txt(filepath)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "def load_documents_from_folder(folder_path):\n",
    "    all_docs = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            text = read_any_file(full_path)\n",
    "            all_docs.append({\n",
    "                \"filename\": file,\n",
    "                \"text\": text\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Could not read {file}: {e}\")\n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Use the same path format as in your existing notebook\n",
    "# doc_folder = \"ami_public_manual_1.6.2/abstractive\"\n",
    "# summaries = load_documents_from_folder(doc_folder)\n",
    "\n",
    "# print(f\"Loaded {len(summaries)} documents\")\n",
    "# print(summaries[0]['filename'])\n",
    "# print(summaries[0]['text'][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c5eb1bd-37ef-4feb-a86f-786912ffc5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5161 files:\n",
      "  00README_MANUAL.txt\n",
      "  AMI-metadata.xml\n",
      "  LICENCE.txt\n",
      "  manifest_1.7.html\n",
      "  MANIFEST_MANUAL.txt\n",
      "  resource.xml\n",
      "  abstractive\\Akshat Mishra Resume DS .pdf\n",
      "  abstractive\\Arsalan_Anwar_Resume (1).pdf\n",
      "  abstractive\\ES2002a.abssumm.xml\n",
      "  abstractive\\ES2002b.abssumm.xml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import html2text\n",
    "import xml.etree.ElementTree as ET\n",
    "from docx import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ── file readers (unchanged) ──────────────────────────────────────────────────\n",
    "def extract_text_from_pdf(path):\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        return \"\\n\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "\n",
    "def extract_text_from_docx(path):\n",
    "    return \"\\n\".join(p.text for p in Document(path).paragraphs)\n",
    "\n",
    "def extract_text_from_html(path):\n",
    "    soup = BeautifulSoup(open(path, encoding=\"utf-8\"), \"html.parser\")\n",
    "    return html2text.html2text(soup.prettify())\n",
    "\n",
    "def extract_text_from_txt(path):\n",
    "    return open(path, encoding=\"utf-8\").read()\n",
    "\n",
    "def extract_text_from_xml(path):\n",
    "    tree = ET.parse(path)\n",
    "    return \" \".join(e.text.strip() for e in tree.getroot().iter() if e.text)\n",
    "\n",
    "def read_any_file(filepath):\n",
    "    ext = os.path.splitext(filepath)[1].lower()\n",
    "    if ext == \".pdf\":      return extract_text_from_pdf(filepath)\n",
    "    if ext == \".docx\":     return extract_text_from_docx(filepath)\n",
    "    if ext == \".html\":     return extract_text_from_html(filepath)\n",
    "    if ext in (\".txt\",\".md\",\".css\"): return extract_text_from_txt(filepath)\n",
    "    if ext == \".xml\":      return extract_text_from_xml(filepath)\n",
    "    raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "# ── recursive loader ───────────────────────────────────────────────────────────\n",
    "def load_all_documents(root_folder):\n",
    "    \"\"\"\n",
    "    Walks every subdirectory under `root_folder`,\n",
    "    reads each supported file, and returns a list of dicts.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        for fname in filenames:\n",
    "            full = os.path.join(dirpath, fname)\n",
    "            try:\n",
    "                text = read_any_file(full)\n",
    "                docs.append({\n",
    "                    \"filename\": os.path.relpath(full, root_folder),  # relative path\n",
    "                    \"text\": text\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Skipped {full}: {e}\")\n",
    "    return docs\n",
    "\n",
    "# ── use it ─────────────────────────────────────────────────────────────────────\n",
    "root = r\"F:\\rag-ami\\ami_public_manual_1.6.2\"  \n",
    "summaries = load_all_documents(root)\n",
    "print(f\"Loaded {len(summaries)} files:\")\n",
    "for doc in summaries[:10]:\n",
    "    print(\" \", doc[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "109e3087-d0cb-4c64-952c-dbff43c704f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39202107-07ee-42b8-ba73-d273f222d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Load GPT-2 tokenizer (used by OpenAI for estimating token counts)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ef9d1c2-59a6-45c2-9af2-f2c5844f0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=500):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for word in words:\n",
    "        token_len = len(tokenizer.encode(word, add_special_tokens=False))\n",
    "        if current_tokens + token_len > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_tokens = token_len\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_tokens += token_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b40df4f6-4619-43e2-9f2e-2b3ad3597d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 3630\n",
      "{'filename': '00README_MANUAL.txt', 'chunk_id': 0, 'text': \"AMI Manual Annotations release 1.7 Date: 16th June 2014 Built by: Jonathan Kilgour Contact: amicorpus@amiproject.org Please read LICENCE.txt before using this data. Please quote the release number in any correspondence. The annotation data is in a format ready to be used directly by NXT. Download and further information here: http://www.ltg.ed.ac.uk/NITE/ This data requires NXT 1.4.1 or later. To use this data with AMI media files, make sure the signals you have downloaded from http://corpus.amiproject.org/ are in a directory called 'signals' under this directory. ------------------------ Changes in public release 1.7 from 1.6 Only one change: transcription files for non-scenario meetings updated to include more accurate and complete timings so that scripts to extract timing information do not return NaN (not a number) results. ------------------------ Changes in public release 1.6 from 1.5 For full list of annotations in this release: see MANIFEST_MANUAL.txt NEW DATA * You-usage annotations for 17 meetings contributed by Matthew Purver of Queen Mary University of London; created by the CALO Project CSLI Stanford team. Transform to NXT: Rieks op den Akker, University of Twente KNOWN ISSUES The meetings IS1002a or IS1005d were dropped completely from the corpus because of serious problems with the audio recrdings. A small number of words are not assigned timings from the forced-alignment process, causing timing propagation to a small number of segments to fail. Timings are known to be incomplete / incorrect for meetings TS3009c (channel 3 only); EN2002a,c; EN2003a. Please report any other timing issues to jonathan at inf.ed.ac.uk. There are 28 dialogue-acts in the corpus that are not associated with any type, due to annotator error. Addressing is part of the dialogue-act annotation and is deliberately only annotated for these meetings: ES2008a,b; ES2009c,d; IS1000a; IS1001a,b,c;\"}\n"
     ]
    }
   ],
   "source": [
    "chunked_data = []\n",
    "\n",
    "for summary in summaries:\n",
    "    chunks = chunk_text(summary[\"text\"], max_tokens=500)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_data.append({\n",
    "            \"filename\": summary[\"filename\"],\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks created: {len(chunked_data)}\")\n",
    "print(chunked_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd03eab7-0477-4001-b5bc-25642173a409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\aksha\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.51.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de861266-52a6-4a1a-b372-c9a8e8b995f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Fast, small, good quality\n",
    "\n",
    "def get_local_embedding(text):\n",
    "    return embedding_model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fffa2b4f-7d60-4259-b6ec-a3a9ff387bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [item['text'] for item in chunked_data]\n",
    "embeddings = [get_local_embedding(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee8ab7bd-3c5b-408d-9462-0b4f9a124585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index contains 3630 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = len(embeddings[0])  # should be 384 for MiniLM\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings).astype('float32'))\n",
    "\n",
    "print(f\"FAISS index contains {index.ntotal} vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbe4c4aa-96ed-43c3-a048-d6a284c73b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f185c54-138f-48fa-9d35-0b6faf4dfd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "TOGETHER_API_KEY = \"cc4b628095c0531f06fe08ff20e1f0bad8cf4e6c39ed2b3c70744a6278a7faab\"  # paste the key from your dashboard\n",
    "\n",
    "def generate_answer(prompt, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"):\n",
    "    url = \"https://api.together.xyz/v1/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "\n",
    "    try:\n",
    "        return result[\"choices\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error parsing Together.ai response:\")\n",
    "        print(result)\n",
    "        return \"No output returned.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af53a848-87f7-4059-b138-a3c6a0712f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, top_k=5):\n",
    "    query_embedding = get_local_embedding(query)\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), top_k)\n",
    "    return [chunked_data[i]['text'] for i in I[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5af8ae5e-40cd-454e-b4b0-df913022c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag_agent(query):\n",
    "    top_chunks = search_faiss(query, top_k=5)\n",
    "    context = \"\\n\\n\".join(top_chunks)\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful meeting assistant.\n",
    "Based on the following context from AMI meeting summaries, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    return generate_answer(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ad992ab-d2f6-4499-b584-c74677db0f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arsalan and Akshat have both worked on developing music recommendation systems and have experience with machine learning algorithms such as KNN and K-means. Additionally, they have both used Python and SQL in their professional experiences. Arsalan has built a music recommendation system for Spotify, while Akshat has designed a music recommendation system for the NYU Data Science Bootcamp. Both of them have achieved high accuracy in personalized suggestions and have enhanced user engagement through advanced clustering and similarity matching algorithms.\n"
     ]
    }
   ],
   "source": [
    "response = ask_rag_agent(\"What is the similarity between Arsalans and Akshats Resume?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f59b2040-3a0a-4bc4-b8d0-7afde3da5bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The main speaker in ES2002b was the User Interface Designer, who presented the major components of the interface design, dividing the interface into two parts: voice commands and buttons. The Marketing Expert was also a key speaker, as she reported on research which shows that users think most remotes are ugly, easily lost and bad for RSI. Audio settings are rarely used, and the power, channel and volume buttons are used most often. The remote should be user-friendly and have a good look and feel.\n",
      "\n",
      "The usability concerns in TS3010b were that remotes were too difficult to use, users want fancier and more ergonomic designs, shock protection, voice recognition, and LCD screens. The Project Manager also announced a new requirement that the remote is only to control televisions. The group decided to eliminate the LCD screen and voice recognition from the design due to time and cost restraints. They also decided to include a previous channel change button to the standard remote buttons, and to have a wheel for changing channels in increments, with a smaller number pad below it. The remote will not have an LCD screen. The remote will not include speech recognition. The group experienced many technical difficulties with their presentations; all participants encountered problems when opening their presentations.\n"
     ]
    }
   ],
   "source": [
    "response = ask_rag_agent(\"Who was the main speaker in ES2002b and What were the usability concerns in TS3010b\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad0bbb9c-62fc-4cc0-9b8e-4a57d95d2c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Action Items from the Meeting:\n",
      "\n",
      "1. Gather more information for the next meeting, the functional design meeting. (All Participants)\n",
      "2. Decide on the inclusion of speech recognition in the design. (All Participants)\n",
      "3. Determine the target group and features to attract them. (All Participants)\n",
      "4. Discuss and decide on the buttons for the remote control. (All Participants)\n",
      "5. Create the design of the remote control. (To be decided in the next meeting)\n",
      "6. Industrial Designer: Work on the working design and technical function.\n",
      "7. User Interface Designer: Work on the working design and functional design.\n",
      "8. Marketing Manager: Look for user requirement specifications such as friendliness, selling price, and profit.\n",
      "9. Consider the possibility of a touch screen, LCD, and other functions. (All Participants)\n",
      "10. Fill out the questionnaire. (All Participants)\n",
      "11. Receive specific instructions for the next meeting by email. (All Participants)\n"
     ]
    }
   ],
   "source": [
    "response = ask_rag_agent(\"TS3010b give the action items of this meeting, and who is responsible for what and write the name of person wherever possible?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615257d-9e8c-4d51-a9f6-ad0adb26312f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
