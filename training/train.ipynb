{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e52ffb9-c2c5-4ad6-b9a6-c552103e571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.12/site-packages (25.0)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.0\n",
      "    Uninstalling pip-25.0:\n",
      "      Successfully uninstalled pip-25.0\n",
      "Successfully installed pip-25.1.1\n",
      "\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --disable-pip-version-check torch torchdata --quiet\n",
    "!pip install transformers datasets evaluate rouge_score loralib peft --quiet\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "331b73e8-427a-414e-9088-6c367572f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df5626-086c-4199-aaa1-dc8212580aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://groups.inf.ed.ac.uk/ami/AMICorpusAnnotations/ami_public_manual_1.6.2.zip\n",
    "!unzip ami_public_manual_1.6.2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff703152-e3ac-4eb8-94f1-1568b08b9702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summarization dataset with 142 entries.\n"
     ]
    }
   ],
   "source": [
    "# AMI Summarization Dataset Preparation Script (Updated for multiple speaker word files)\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Parse words file to extract and reconstruct transcript\n",
    "def parse_words_file(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    words = []\n",
    "    for w in root.findall(\".//w\"):\n",
    "        start_time = float(w.attrib.get(\"starttime\", 0.0))\n",
    "        end_time = float(w.attrib.get(\"endtime\", 0.0))\n",
    "        text = w.text or \"\"\n",
    "        words.append((start_time, end_time, text))\n",
    "    return words\n",
    "\n",
    "def build_transcript(words):\n",
    "    words.sort(key=lambda x: x[0])\n",
    "    return \" \".join(word for _, _, word in words)\n",
    "\n",
    "# Parse summary XML file to extract human-written abstract, actions, and decisions\n",
    "def parse_summary_xml(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    def extract_sentences(section):\n",
    "        if section is None:\n",
    "            return \"\"\n",
    "        return \" \".join([s.text for s in section.findall(\"sentence\") if s.text and s.text.strip() != \"*NA*\"])\n",
    "\n",
    "    abstract = extract_sentences(root.find(\".//abstract\"))\n",
    "    actions = extract_sentences(root.find(\".//actions\"))\n",
    "    decisions = extract_sentences(root.find(\".//decisions\"))\n",
    "    combined_summary = \" \".join([abstract, actions, decisions])\n",
    "    return combined_summary\n",
    "\n",
    "# Process all meetings in AMI dataset\n",
    "def process_all_ami_meetings(base_path):\n",
    "    words_dir = os.path.join(base_path, \"words\")\n",
    "    abssumm_dir = os.path.join(base_path, \"abstractive\")\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for fname in os.listdir(abssumm_dir):\n",
    "        if not fname.endswith(\".xml\"):\n",
    "            continue\n",
    "\n",
    "        meeting_id = fname.split(\".\")[0]  # e.g., TS3010b\n",
    "        summary_path = os.path.join(abssumm_dir, fname)\n",
    "\n",
    "        # Collect all speaker word files: TS3010b.A.words.xml, TS3010b.B.words.xml, etc.\n",
    "        meeting_word_files = [\n",
    "            os.path.join(words_dir, f)\n",
    "            for f in os.listdir(words_dir)\n",
    "            if f.startswith(meeting_id + \".\") and f.endswith(\".words.xml\")\n",
    "        ]\n",
    "\n",
    "        if not meeting_word_files:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Combine transcripts from all channels\n",
    "            full_word_list = []\n",
    "            for wp in meeting_word_files:\n",
    "                full_word_list.extend(parse_words_file(wp))\n",
    "            transcript = build_transcript(full_word_list)\n",
    "\n",
    "            # Parse summary\n",
    "            summary = parse_summary_xml(summary_path)\n",
    "\n",
    "            # Add to dataset\n",
    "            dataset.append({\n",
    "                \"meeting_id\": meeting_id,\n",
    "                \"input_text\": transcript[:3000],  # Trim for practicality\n",
    "                \"target_summary\": summary\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {meeting_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(dataset)\n",
    "\n",
    "ami_base_dir = \"\"  # Update this path\n",
    "summarization_df = process_all_ami_meetings(ami_base_dir)\n",
    "summarization_df.to_csv(\"ami_summarization_dataset.csv\", index=False)\n",
    "print(\"Saved summarization dataset with\", len(summarization_df), \"entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f560e13c-f374-4ca5-b1ae-69d5e0e03d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manually with pandas\n",
    "df = pd.read_csv(\"ami_summarization_dataset.csv\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "024be264-73f8-4866-850a-9b83f682ca46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face token:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "from huggingface_hub import login\n",
    "\n",
    "token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a9f81a9-0433-4a27-81e6-8a6c44383083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e8c387ed5a4ed7a0ef6455acc8f6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a834a0f-0cf4-4f54-925c-b88a122f49ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3cae3b739647c08b194239df647d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8e74299f9f4285b636f2b3f61f8b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize this meeting transcript in 1-2 factual sentences:.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"input_text\"]]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "    # Tokenize targets (labels)\n",
    "    labels = tokenizer(example[\"target_summary\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "    # Replace padding token id's in labels with -100 so they are ignored in loss\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    # Combine into a single return dict\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply to both splits\n",
    "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_test = dataset[\"test\"].map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove original columns from each\n",
    "columns_to_remove = dataset[\"train\"].column_names\n",
    "tokenized_train = tokenized_train.remove_columns(columns_to_remove)\n",
    "tokenized_test = tokenized_test.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be0284e9-07a4-4beb-8c64-dd005912958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (127, 3)\n",
      "Test: (15, 3)\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 127\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_train.shape}\")\n",
    "print(f\"Test: {tokenized_test.shape}\")\n",
    "print(tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82133297-27fd-4575-8a34-bd2d67481e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1bcb31f8-e7e8-4f17-8be0-bdf143b63b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 7241732096\n",
      "all model parameters: 7241732096\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b082643-8bb1-49a8-8e76-3d1cf0075e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(transcript: str, max_new_tokens: int = 80) -> str:\n",
    "    prompt = f\"Summarize this meeting transcript in 1-2 factual sentences:. Transcript:{transcript.strip()}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = peft_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1.5,\n",
    "            length_penalty=1.0\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"Summary:\")[-1].strip()\n",
    "\n",
    "import re\n",
    "def truncate_after_n_sentences(text, n=4):\n",
    "    \"\"\"\n",
    "    Truncate a string after n full sentences. Keeps punctuation.\n",
    "    \"\"\"\n",
    "    # Matches sentences ending with ., !, or ?, optionally followed by quotes or whitespace\n",
    "    sentence_endings = re.findall(r'[^.!?]*[.!?][\"\\']?\\s*', text)\n",
    "\n",
    "    # Take the first n non-empty sentences\n",
    "    non_empty = [s.strip() for s in sentence_endings if s.strip()]\n",
    "    truncated = ' '.join(non_empty[:n])\n",
    "    return truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9d312-61a6-4d8b-ae73-0bcbdb7f533c",
   "metadata": {},
   "source": [
    "## Configuration - 1 (Mistral 7B fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fcfc62-00ae-4ea2-8b0b-cab415e5cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c27bb7-aaaa-4143-95a6-93613dd1358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize this meeting transcript in 1-2 factual sentences:.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"input_text\"]]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "    # Tokenize targets (labels)\n",
    "    labels = tokenizer(example[\"target_summary\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "    # Replace padding token id's in labels with -100 so they are ignored in loss\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    # Combine into a single return dict\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply to both splits\n",
    "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_test = dataset[\"test\"].map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove original columns from each\n",
    "columns_to_remove = dataset[\"train\"].column_names\n",
    "tokenized_train = tokenized_train.remove_columns(columns_to_remove)\n",
    "tokenized_test = tokenized_test.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "92e14c15-ccbb-4159-b96b-7be0f30faaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./meeting-summary-training_mistral'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fbb08fc4-edec-4396-aad8-8cb7d805863c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 12:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.995100</td>\n",
       "      <td>6.055297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.872700</td>\n",
       "      <td>6.006083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.339100</td>\n",
       "      <td>6.050463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.218800</td>\n",
       "      <td>6.802176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.965600</td>\n",
       "      <td>7.453739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=4.890133386850357, metrics={'train_runtime': 742.8466, 'train_samples_per_second': 0.855, 'train_steps_per_second': 0.431, 'total_flos': 6935453431234560.0, 'train_loss': 4.890133386850357, 'epoch': 5.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2272d562-b073-4ca6-b820-a5924a11e143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize this meeting transcript in 1-2 factual sentences:. Transcript:Hi everyone! I just finished writing the dockerfiles for the CI/CD pipeline. What about you? I have built the Prometheus and Grafana dashboards. We now need to integrate the same into our overall pipeline. Yeah, we also need to add Kubernetes to it. What about the model training and MLFlow implementation? I've started working on it, will also add Ray Tune. Cool, let us meet later.\n",
      "\n",
      "=== Generated Summary ===\n",
      "\n",
      "m\u0000\u0006 N���� reflection certain�������ton����able Bar\u0002 mY\u0000\u0006ter��op�������ities\u0003AL? ector\tirst\u001f\u0002 m,M. .\n"
     ]
    }
   ],
   "source": [
    "# === Example Usage === #\n",
    "\n",
    "test_transcript = \"\"\"\n",
    "Hi everyone! I just finished writing the dockerfiles for the CI/CD pipeline. What about you? I have built the Prometheus and Grafana dashboards. We now need to integrate the same into our overall pipeline. Yeah, we also need to add Kubernetes to it. What about the model training and MLFlow implementation? I've started working on it, will also add Ray Tune. Cool, let us meet later.\n",
    "    \"\"\"\n",
    "\n",
    "summary = generate_summary(test_transcript)\n",
    "print(\"\\n=== Generated Summary ===\\n\")\n",
    "print(truncate_after_n_sentences(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e422c5-c896-4fd1-9920-07c53717d966",
   "metadata": {},
   "source": [
    "## Configuration - 2 (Flan-T5-Large fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31b8272f-3f6a-44a8-ba4d-8155a0408ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-large'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
    "torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7c5be4af-e83b-4e06-8200-18010f97dbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704d15bae4a845e888f99361530b8382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d9981adf444cec800ab2671153c98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize this meeting transcript in 1-2 factual sentences:.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"input_text\"]]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "    # Tokenize targets (labels)\n",
    "    labels = tokenizer(example[\"target_summary\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "    # Replace padding token id's in labels with -100 so they are ignored in loss\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    # Combine into a single return dict\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply to both splits\n",
    "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_test = dataset[\"test\"].map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove original columns from each\n",
    "columns_to_remove = dataset[\"train\"].column_names\n",
    "tokenized_train = tokenized_train.remove_columns(columns_to_remove)\n",
    "tokenized_test = tokenized_test.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "527e3e3a-9b3a-46ea-9662-81dfc3d9d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./meeting-summary-training_flan'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06bc9222-98fb-432c-a722-f845a0c97570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 01:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.318800</td>\n",
       "      <td>3.221875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.476600</td>\n",
       "      <td>3.206250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.273400</td>\n",
       "      <td>3.198958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.423400</td>\n",
       "      <td>3.198958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.403100</td>\n",
       "      <td>3.198958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=3.38037109375, metrics={'train_runtime': 98.7005, 'train_samples_per_second': 6.434, 'train_steps_per_second': 3.242, 'total_flos': 731763824394240.0, 'train_loss': 3.38037109375, 'epoch': 5.0})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0a5ef88-1d07-467b-a2eb-63672e8b1e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize this meeting transcript in 1-2 factual sentences:. Transcript:Hi everyone! I just finished writing the dockerfiles for the CI/CD pipeline. What about you? I have built the Prometheus and Grafana dashboards. We now need to integrate the same into our overall pipeline. Yeah, we also need to add Kubernetes to it. What about the model training and MLFlow implementation? I've started working on it, will also add Ray Tune. Cool, let us meet later.\n",
      "\n",
      "=== Generated Summary ===\n",
      "\n",
      "The project manager opened the meeting by stating that the team had just finished writing the dockerfiles for the CI/CD pipeline and that they needed to add Kubernetes, Prometheus, Grafana, and MLFlow to the overall pipeline. The team members then discussed their roles in the project and what they would like to accomplish in the next meeting.\n"
     ]
    }
   ],
   "source": [
    "# === Example Usage === #\n",
    "\n",
    "test_transcript = \"\"\"\n",
    "Hi everyone! I just finished writing the dockerfiles for the CI/CD pipeline. What about you? I have built the Prometheus and Grafana dashboards. We now need to integrate the same into our overall pipeline. Yeah, we also need to add Kubernetes to it. What about the model training and MLFlow implementation? I've started working on it, will also add Ray Tune. Cool, let us meet later.\n",
    "    \"\"\"\n",
    "\n",
    "summary = generate_summary(test_transcript)\n",
    "print(\"\\n=== Generated Summary ===\\n\")\n",
    "print(truncate_after_n_sentences(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73223ea5-0110-4dc5-9dc3-fcf59ec7e951",
   "metadata": {},
   "source": [
    "## Configuration - 3 (Flan-T5-Large with LoRA with lr 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2edccb-371c-4b49-bd50-4d80cc90326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534fc6a-8e13-4f71-882a-1de55d4139a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "if isinstance(peft_model, PeftModel):\n",
    "    peft_model = peft_model.unload()\n",
    "\n",
    "peft_model = get_peft_model(model,\n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a02b46f-6e12-4820-863a-245002b366ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./meeting-summary-training2'\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    )\n",
    "\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6abb751-42e8-4b9c-9a0e-f4231172056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 01:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.245300</td>\n",
       "      <td>3.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.268700</td>\n",
       "      <td>3.001042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.967200</td>\n",
       "      <td>2.947917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.064100</td>\n",
       "      <td>2.920833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.068800</td>\n",
       "      <td>2.911458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=3.146435546875, metrics={'train_runtime': 76.1405, 'train_samples_per_second': 8.34, 'train_steps_per_second': 4.203, 'total_flos': 734064987340800.0, 'train_loss': 3.146435546875, 'epoch': 5.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21eb00be-16cb-439b-956d-982c95dba789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize this meeting transcript in 1-2 factual sentences:. Transcript:Hi everyone! I just finished writing the dockerfiles for the CI/CD pipeline. What about you? I have built the Prometheus and Grafana dashboards. We now need to integrate the same into our overall pipeline. Yeah, we also need to add Kubernetes to it. What about the model training and MLFlow implementation? I've started working on it, will also add Ray Tune. Cool, let us meet later.\n",
      "\n",
      "=== Generated Summary ===\n",
      "\n",
      "We discussed the CI/CD pipeline, Prometheus and Grafana dashboards, Kubernetes, model training and MLFlow implementation.\n"
     ]
    }
   ],
   "source": [
    "# === Example Usage === #\n",
    "\n",
    "test_transcript = \"\"\"\n",
    "Hi everyone! I just finished writing the dockerfiles for the CI/CD pipeline. What about you? I have built the Prometheus and Grafana dashboards. We now need to integrate the same into our overall pipeline. Yeah, we also need to add Kubernetes to it. What about the model training and MLFlow implementation? I've started working on it, will also add Ray Tune. Cool, let us meet later.\n",
    "    \"\"\"\n",
    "\n",
    "summary = generate_summary(test_transcript)\n",
    "print(\"\\n=== Generated Summary ===\\n\")\n",
    "print(truncate_after_n_sentences(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d67e2-97c9-4188-8592-bc346746cb75",
   "metadata": {},
   "source": [
    "## Configuration - 4 (Flan-T5-Large with LoRA with lr 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1243a5fb-d002-49bf-8377-efeed06e0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3660e98-8918-41f8-ade7-87f24d6035af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 2359296\n",
      "all model parameters: 785509376\n",
      "percentage of trainable model parameters: 0.30%\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "if isinstance(peft_model, PeftModel):\n",
    "    peft_model = peft_model.unload()\n",
    "\n",
    "peft_model = get_peft_model(model,\n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ec57939-5fc3-4cc0-8fb7-5ef7148b5ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./meeting-summary-training3'\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",     \n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    )\n",
    "\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcba08bb-23a4-4236-b4cc-ff4ee0b3aaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 01:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.876600</td>\n",
       "      <td>2.720833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.740600</td>\n",
       "      <td>2.595833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.376600</td>\n",
       "      <td>2.517708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.368000</td>\n",
       "      <td>2.477083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.414100</td>\n",
       "      <td>2.467708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=2.59228515625, metrics={'train_runtime': 76.1553, 'train_samples_per_second': 8.338, 'train_steps_per_second': 4.202, 'total_flos': 734064987340800.0, 'train_loss': 2.59228515625, 'epoch': 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b39b93cd-8a27-48e0-8b3a-b7c6c5e58267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize this meeting transcript in 1-2 factual sentences:. Transcript:Hi everyone! I just finished writing the dockerfiles for the CI/CD pipeline. What about you? I have built the Prometheus and Grafana dashboards. We now need to integrate the same into our overall pipeline. Yeah, we also need to add Kubernetes to it. What about the model training and MLFlow implementation? I've started working on it, will also add Ray Tune. Cool, let us meet later.\n",
      "\n",
      "=== Generated Summary ===\n",
      "\n",
      "The project manager opened the meeting by introducing himself to the group and stating the agenda for the next meeting. The industrial designer presented his prototype of a remote control device that can be used to control a television. The marketing expert discussed the current trends in remote control devices and discussed how to incorporate them into the design of the remote.\n"
     ]
    }
   ],
   "source": [
    "# === Example Usage === #\n",
    "\n",
    "test_transcript = \"\"\"\n",
    "Hi everyone! I just finished writing the dockerfiles for the CI/CD pipeline. What about you? I have built the Prometheus and Grafana dashboards. We now need to integrate the same into our overall pipeline. Yeah, we also need to add Kubernetes to it. What about the model training and MLFlow implementation? I've started working on it, will also add Ray Tune. Cool, let us meet later.\n",
    "    \"\"\"\n",
    "\n",
    "summary = generate_summary(test_transcript)\n",
    "print(\"\\n=== Generated Summary ===\\n\")\n",
    "print(truncate_after_n_sentences(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9693f-1f38-4686-af58-c56f0126b77d",
   "metadata": {},
   "source": [
    "## Configuration - 5 (Flan-T5-Large with LoRA with lr 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e372a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9dd68593-4225-41d3-a8fe-429b6a3dc738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 2359296\n",
      "all model parameters: 785509376\n",
      "percentage of trainable model parameters: 0.30%\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "if isinstance(peft_model, PeftModel):\n",
    "    peft_model = peft_model.unload()\n",
    "\n",
    "peft_model = get_peft_model(model,\n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b22ce1e8-2df6-4dd4-ab7c-c5aee23f3b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./meeting-summary-training4'\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",      \n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    )\n",
    "\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d8e44f97-4f01-49c2-a35d-3ae67a646486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 01:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.223400</td>\n",
       "      <td>3.073958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.245300</td>\n",
       "      <td>2.985417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.945300</td>\n",
       "      <td>2.932292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.046900</td>\n",
       "      <td>2.905208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>2.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=3.122119140625, metrics={'train_runtime': 77.9665, 'train_samples_per_second': 8.145, 'train_steps_per_second': 4.104, 'total_flos': 734064987340800.0, 'train_loss': 3.122119140625, 'epoch': 5.0})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d5a33993-c7a1-4def-b6c8-eb012bc1e5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize this meeting transcript in 1-2 factual sentences:. Transcript:Hi everyone! I just finished writing the dockerfiles for the CI/CD pipeline. What about you? I have built the Prometheus and Grafana dashboards. We now need to integrate the same into our overall pipeline. Yeah, we also need to add Kubernetes to it. What about the model training and MLFlow implementation? I've started working on it, will also add Ray Tune. Cool, let us meet later.\n",
      "\n",
      "=== Generated Summary ===\n",
      "\n",
      "We discussed the CI/CD pipeline, Prometheus and Grafana dashboards, Kubernetes, model training and MLFlow implementation.\n"
     ]
    }
   ],
   "source": [
    "# === Example Usage === #\n",
    "\n",
    "test_transcript = \"\"\"\n",
    "Hi everyone! I just finished writing the dockerfiles for the CI/CD pipeline. What about you? I have built the Prometheus and Grafana dashboards. We now need to integrate the same into our overall pipeline. Yeah, we also need to add Kubernetes to it. What about the model training and MLFlow implementation? I've started working on it, will also add Ray Tune. Cool, let us meet later.\n",
    "    \"\"\"\n",
    "\n",
    "summary = generate_summary(test_transcript)\n",
    "print(\"\\n=== Generated Summary ===\\n\")\n",
    "print(truncate_after_n_sentences(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3ee6b-923d-4d0d-a5ed-d58ebbe74f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
